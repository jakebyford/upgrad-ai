{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_mistralai faiss-cpu langchain-community langchain-openai openai==1.55.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDKuW82YWty3",
        "outputId": "35720d23-6753-4094-eef4-67bdbb5e7613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Collecting langchain_mistralai\n",
            "  Downloading langchain_mistralai-0.2.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting openai==1.55.3\n",
            "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (0.28.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (0.8.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.9)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.21)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting httpx-sse<1,>=0.3.1 (from langchain_mistralai)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from langchain_mistralai) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.21 (from langchain)\n",
            "  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.55.3) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.55.3) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.55.3) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.55.3) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.55.3) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (2.27.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15.1->langchain_mistralai) (0.26.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (2024.10.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_mistralai-0.2.3-py3-none-any.whl (15 kB)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, tiktoken, pydantic-settings, openai, dataclasses-json, langchain-core, langchain-openai, langchain_mistralai, langchain, langchain-community\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.21\n",
            "    Uninstalling langchain-core-0.3.21:\n",
            "      Successfully uninstalled langchain-core-0.3.21\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.9\n",
            "    Uninstalling langchain-0.3.9:\n",
            "      Successfully uninstalled langchain-0.3.9\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.9.0.post1 httpx-sse-0.4.0 langchain-0.3.11 langchain-community-0.3.11 langchain-core-0.3.24 langchain-openai-0.2.12 langchain_mistralai-0.2.3 marshmallow-3.23.1 mypy-extensions-1.0.0 openai-1.55.3 pydantic-settings-2.6.1 python-dotenv-1.0.1 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#demo 1"
      ],
      "metadata": {
        "id": "lYd_o93aOHwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QLh5cm_MNtS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "# from langchain.chat_models import\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "embedding_model=OpenAIEmbeddings(api_key=userdata.get('open_api_key'))\n",
        "model=ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=userdata.get('open_api_key'))\n",
        "\n",
        "def fetch_and_load_documents():\n",
        "  dataset_path=\"dataset/vector-retriever-sample2.txt\"\n",
        "  if not os.path.exists(dataset_path):\n",
        "    os.makedirs(\"dataset\",exist_ok=True)\n",
        "\n",
        "    with open(dataset_path,\"w\") as f:\n",
        "      f.write(\n",
        "          \"AI is transforming industries like healthcare, finance and education.\\n\"\n",
        "          \"Artificial intelligence helps to automate repetitive tasks\\n\"\n",
        "          \"AI applications imclude natural language processing and computer vision tasks\\n\"\n",
        "          \"Deep learning has advanced AI significantly in recent years\\n\"\n",
        "          \"AI raises ethical concerns, including bias and privacy issues\"\n",
        "      )\n",
        "\n",
        "  print(\"dataset saved\")\n",
        "  loader=TextLoader(dataset_path)\n",
        "  documents=loader.load()\n",
        "  return documents\n"
      ],
      "metadata": {
        "id": "PlvuRXslLLvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_faiss_index(documents):\n",
        "  print(\"creating faiss index\")\n",
        "  vector_db=FAISS.from_documents(documents, embedding_model)\n",
        "  vector_db.save_local(\"faiss_indexes/vector_storew_retriever\")\n",
        "  return vector_db"
      ],
      "metadata": {
        "id": "scmolnOSQdjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_search(vector_db,query,k=3):\n",
        "  print(\"preforming similarity search\")\n",
        "  retriever=vector_db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":k})\n",
        "  return retriever.get_relevant_documents(query)"
      ],
      "metadata": {
        "id": "_9cGOA8tRa5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=fetch_and_load_documents()\n",
        "vector_db=create_faiss_index(documents)\n",
        "query=\"how does ai help industries?\"\n",
        "results=similarity_search(vector_db,query,k=3)\n",
        "for i,doc in enumerate(results):\n",
        "  print(f\"results{i+1}: {doc.page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py6gCAYdSzR7",
        "outputId": "4ffd364b-e772-4c42-bc3a-b76e7eddc6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset saved\n",
            "creating faiss index\n",
            "preforming similarity search\n",
            "results1: AI is transforming industries like healthcare, finance and education.\n",
            "Artificial intelligence helps to automate repetitive tasks\n",
            "AI applications imclude natural language processing and computer vision tasks\n",
            "Deep learning has advanced AI significantly in recent years\n",
            "AI raises ethical concerns, including bias and privacy issues\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-bab69c9e7689>:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  return retriever.get_relevant_documents(query)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "model.invoke([HumanMessage(\"hi how are you\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT_dUrq5TFQ-",
        "outputId": "6f861f9f-6018-41c2-fa21-4650c26262ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 11, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bba3c8e70b', 'finish_reason': 'stop', 'logprobs': None}, id='run-9a4b3c93-879e-4a5e-8180-3c434e691609-0', usage_metadata={'input_tokens': 11, 'output_tokens': 30, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xF-TR-zVjvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demo 2"
      ],
      "metadata": {
        "id": "JiOfrC8X-OjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "Co6YI-Lz-Pp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model=OpenAIEmbeddings(api_key=userdata.get('open_api_key'))\n",
        "model=ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=userdata.get('open_api_key'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5ozGcULBcv1",
        "outputId": "b55e2664-d584-4705-c2a8-cdcfeebed0cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-98be77c10e36>:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  model=ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=userdata.get('open_api_key'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents():\n",
        "  loader=TextLoader(\"dataset/vector-retriever-sample.txt\")\n",
        "  documents=loader.load()\n",
        "  return documents"
      ],
      "metadata": {
        "id": "lNY8atoCBsPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store_embeddings(documents):\n",
        "  vector_db=FAISS.from_documents(documents, embedding_model)\n",
        "  return vector_db"
      ],
      "metadata": {
        "id": "oXX4M0CZB5jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_and_generate(vector_db,query):\n",
        "  retriever=vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
        "  qa_chain=RetrievalQA.from_chain_type(model, retriever=retriever)\n",
        "  answer=qa_chain.invoke(query)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "9YNCnWn2CBlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_and_load_documents():\n",
        "  dataset_path=\"dataset/vector-retriever-sample.txt\"\n",
        "  if not os.path.exists(dataset_path):\n",
        "    os.makedirs(\"dataset\",exist_ok=True)\n",
        "\n",
        "    with open(dataset_path,\"w\") as f:\n",
        "      f.write(\n",
        "          \"AI is transforming industries like healthcare, finance and education.\"\n",
        "      )\n",
        "fetch_and_load_documents()"
      ],
      "metadata": {
        "id": "ekd4-w1rC5rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=load_documents()\n",
        "vector_db=store_embeddings(documents)\n",
        "\n",
        "answer=query_and_generate(vector_db, \"what doesthe document say about the future of ai?\")\n",
        "print(f\"generated answer:{answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "794g_oSQCVpJ",
        "outputId": "7c2c7554-e60b-41ad-af89-9874f8395d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated answer:{'query': 'what doesthe document say about the future of ai?', 'result': 'The document suggests that AI is transforming industries like healthcare, finance, and education. It implies that AI will continue to play a significant role in shaping the future of various sectors.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dz-ndUENCtBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demo 3 with memory"
      ],
      "metadata": {
        "id": "_1WPrqcrN6GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "pCFnPFRQN76W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model=OpenAIEmbeddings(api_key=userdata.get('open_api_key'))\n",
        "model=ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=userdata.get('open_api_key'))"
      ],
      "metadata": {
        "id": "smCW9H4lOOl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_and_load_documents():\n",
        "    dataset_path = \"dataset/conversational_faq_sample.txt\"\n",
        "    if not os.path.exists(dataset_path):\n",
        "        os.makedirs(\"dataset\", exist_ok=True)\n",
        "        with open(dataset_path, \"w\") as f:\n",
        "            f.write(\n",
        "                \"Q: What is AI?\\n\"\n",
        "                \"A: AI, or Artificial Intelligence, refers to the simulation of human intelligence in machines.\\n\"\n",
        "                \"Q: What are some applications of AI?\\n\"\n",
        "                \"A: Applications include natural language processing, computer vision, and robotics.\\n\"\n",
        "                \"Q: What industries benefit from AI?\\n\"\n",
        "                \"A: Industries like healthcare, finance, and education benefit greatly from AI.\\n\"\n",
        "                \"Q: What are ethical concerns in AI?\\n\"\n",
        "                \"A: Ethical concerns include bias, privacy, and decision-making transparency.\"\n",
        "            )\n",
        "    loader = TextLoader(dataset_path)\n",
        "    documents = loader.load()\n",
        "    return documents"
      ],
      "metadata": {
        "id": "edh7Q_0NOWgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_faiss_index(documents):\n",
        "  vector_db=FAISS.from_documents(documents, embedding_model)\n",
        "  vector_db.save_local(\"vector_db\")\n",
        "  return vector_db"
      ],
      "metadata": {
        "id": "D4qSamF4OZ7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversational_retrieval(vector_db, query, search_type='similarity', k=3, threshold=None):\n",
        "  memory=ConversationBufferMemory(memory_type=\"chat_history\", return_messages=True)\n",
        "  search_kwargs={\"k\":k}\n",
        "  if search_type==\"similarity_score_threshold\":\n",
        "    search_kwargs[\"score_threshold\"]=threshold or 0.5\n",
        "  retriever=vector_db.as_retriever(search_type=search_type,search_kwargs=search_kwargs)\n",
        "  retrieval_chain=RetrievalQA.from_chain_type(model, retriever=retriever, memory=memory)\n",
        "  answer=retrieval_chain.invoke({\"query\":query})\n",
        "  return answer\n"
      ],
      "metadata": {
        "id": "p8FKHDOYOnzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=fetch_and_load_documents()\n",
        "vector_db=create_faiss_index(documents)\n",
        "query=\"tell me about ai applications\"\n",
        "answer=conversational_retrieval(vector_db, query, search_type='similarity',k=3)\n",
        "print(answer)\n",
        "print('\\n')\n",
        "answer=conversational_retrieval(vector_db, query, search_type='similarity_score_threshold',k=3,threshold=0.6)\n",
        "print(answer)\n",
        "print('\\n')\n",
        "answer=conversational_retrieval(vector_db, query, search_type='mmr',k=3)\n",
        "print(answer)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2KzxbZsPmzo",
        "outputId": "4a3d6b70-fef6-47fb-8554-5b987731c77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-c2cd73228e86>:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory=ConversationBufferMemory(memory_type=\"chat_history\", return_messages=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'tell me about ai applications', 'history': [HumanMessage(content='tell me about ai applications', additional_kwargs={}, response_metadata={}), AIMessage(content='Some applications of AI include natural language processing, computer vision, and robotics.', additional_kwargs={}, response_metadata={})], 'result': 'Some applications of AI include natural language processing, computer vision, and robotics.'}\n",
            "\n",
            "\n",
            "{'query': 'tell me about ai applications', 'history': [HumanMessage(content='tell me about ai applications', additional_kwargs={}, response_metadata={}), AIMessage(content='AI applications include natural language processing, computer vision, and robotics.', additional_kwargs={}, response_metadata={})], 'result': 'AI applications include natural language processing, computer vision, and robotics.'}\n",
            "\n",
            "\n",
            "{'query': 'tell me about ai applications', 'history': [HumanMessage(content='tell me about ai applications', additional_kwargs={}, response_metadata={}), AIMessage(content='AI applications include natural language processing, computer vision, and robotics.', additional_kwargs={}, response_metadata={})], 'result': 'AI applications include natural language processing, computer vision, and robotics.'}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "egzpAVMLQVWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#semantic search"
      ],
      "metadata": {
        "id": "OhpBJDIzNyMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai pymupdf faiss-cpu wikipedia-api langchain-community langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z-uciOAN0fa",
        "outputId": "4515e060-9b6e-445d-8941-4f8dbc4bc3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.5)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.9)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.21)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.21 (from langchain)\n",
            "  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.57.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading pymupdf-1.25.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.57.2-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.9/389.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=6c30250e64340cb502185460e4d018d452671b7bd86ecb6937a078b06a5d4851\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: python-dotenv, pymupdf, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, wikipedia-api, typing-inspect, tiktoken, pydantic-settings, openai, dataclasses-json, langchain-core, langchain-openai, langchain, langchain-community\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.21\n",
            "    Uninstalling langchain-core-0.3.21:\n",
            "      Successfully uninstalled langchain-core-0.3.21\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.9\n",
            "    Uninstalling langchain-0.3.9:\n",
            "      Successfully uninstalled langchain-0.3.9\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.9.0.post1 httpx-sse-0.4.0 langchain-0.3.11 langchain-community-0.3.11 langchain-core-0.3.24 langchain-openai-0.2.12 marshmallow-3.23.1 mypy-extensions-1.0.0 openai-1.57.2 pydantic-settings-2.6.1 pymupdf-1.25.0 python-dotenv-1.0.1 tiktoken-0.8.0 typing-inspect-0.9.0 wikipedia-api-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install langchain openai pymupdf faiss-cpu wikipedia-api langchain-community langchain-openai\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import wikipediaapi\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, api_key=userdata.get('open_api_key'))\n",
        "embedding_model = OpenAIEmbeddings(api_key=userdata.get('open_api_key'))\n",
        "\n",
        "\n",
        "wiki_api = wikipediaapi.Wikipedia(\n",
        "    language='en',\n",
        "    user_agent=\"MyRAGApp/1.0 (https://upgrad.com; contact@upgrad.com)\"\n",
        ")\n",
        "\n",
        "\n",
        "def load_and_chunk_pdf(file_path, chunk_size=500, chunk_overlap=100):\n",
        "\n",
        "    print(\"Loading and Splitting the PDF Document..\")\n",
        "\n",
        "    loader =PyMuPDFLoader(file_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
        "    )\n",
        "\n",
        "    chunk = text_splitter.split_documents(documents)\n",
        "\n",
        "    print(f\"Number of chunks: {len(chunk)}\")\n",
        "    return chunk\n",
        "\n",
        "\n",
        "def create_vector_database(chunks):\n",
        "    print(\"Creating FAISS Index...\")\n",
        "    vector_db = FAISS.from_documents(chunks, embedding_model)\n",
        "    print(\"FAISS Index Created\")\n",
        "    return vector_db\n",
        "\n",
        "def query_wikipedia(query):\n",
        "    print(f\"Searching for {query} in Wikipedia...\")\n",
        "\n",
        "    page = wiki_api.page(query)\n",
        "\n",
        "    if page.exists:\n",
        "        return page.text[:1000]\n",
        "    else:\n",
        "        return \"No Wikipedia information found\"\n",
        "\n",
        "\n",
        "def structured_response(query, retrievals, wiki_data):\n",
        "\n",
        "    print(\"Preparing Structured Response...\")\n",
        "\n",
        "    response_schema = [\n",
        "\n",
        "        ResponseSchema(name=\"summary\", description=\"A concise summary of the wikipedia page\"),\n",
        "        ResponseSchema(name=\"key_points\", description=\"The key points relevant to the query\"),\n",
        "        ResponseSchema(name=\"wikipedia_reference\", description=\"Relevant information retrieved from Wikipedia\")\n",
        "\n",
        "    ]\n",
        "\n",
        "    parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
        "    format_instructions = parser.get_format_instructions()\n",
        "\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrievals])\n",
        "\n",
        "    response = llm.predict(\n",
        "        f\"\"\"You are an expert assistant. Based on the following context, generate a structured response:\n",
        "        Context: {context}\n",
        "        Wikipedia: {wiki_data}\n",
        "        {format_instructions}\"\"\"\n",
        "    )\n",
        "\n",
        "    return parser.parse(response)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\" Starting Complete RAG Demo \")\n",
        "\n",
        "    file_path = \"./AI_in_healthcare.pdf\"\n",
        "    chunks = load_and_chunk_pdf(file_path)\n",
        "    vector_db = create_vector_database(chunks)\n",
        "\n",
        "\n",
        "    queries = [\n",
        "       \"What examples of AI-driven solutions in clinical care are given?\",\n",
        "    ]\n",
        "\n",
        "    query = queries[0]\n",
        "\n",
        "    retriever = vector_db.as_retriever(\n",
        "        search_type = \"similarity\",\n",
        "        search_kwargs = {\"k\": 3}\n",
        "    )\n",
        "\n",
        "    retrievals = retriever.get_relevant_documents(query)\n",
        "\n",
        "    wiki_data = query_wikipedia(query)\n",
        "\n",
        "    structure_answer = structured_response(query, retrievals, wiki_data)\n",
        "\n",
        "    print(f\"\\n\\n Structured Response: *******************************\")\n",
        "    print(structure_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo14oyxhe7NY",
        "outputId": "f50dae07-73b3-407a-dbf1-0a4949166f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Starting Complete RAG Demo \n",
            "Loading and Splitting the PDF Document..\n",
            "Number of chunks: 90\n",
            "Creating FAISS Index...\n",
            "FAISS Index Created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-dfa0dc347dac>:109: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrievals = retriever.get_relevant_documents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for What examples of AI-driven solutions in clinical care are given? in Wikipedia...\n",
            "Preparing Structured Response...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-dfa0dc347dac>:80: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm.predict(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Structured Response: *******************************\n",
            "{'summary': 'AI technologies are being used or trialled for a range of purposes in healthcare and research, including disease detection, chronic condition management, health service delivery, and drug discovery. While AI has the potential to address health challenges, it may be limited by data quality and the inability to possess human characteristics like compassion.', 'key_points': \"AI in clinical care includes medical imaging analysis, disease detection, chronic condition management, health service delivery, drug discovery, monitoring patient adherence to medication, and early intervention through sensor analysis. Challenges include data quality and AI's lack of human characteristics.\", 'wikipedia_reference': 'https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Function to clip the video into intervals and save in sequential order (video_1, video_2, etc.)\n",
        "def clip_video_in_intervals(video_path, clip_duration, output_folder):\n",
        "    # Ensure the output folder exists\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Extract video name without extension\n",
        "    video_name = os.path.basename(video_path).split('.')[0]\n",
        "\n",
        "    # Create a folder for this video in the output folder\n",
        "    video_output_folder = os.path.join(output_folder, video_name)\n",
        "    if not os.path.exists(video_output_folder):\n",
        "        os.makedirs(video_output_folder)\n",
        "\n",
        "    # Load the video\n",
        "    if os.path.exists(video_path):\n",
        "        video = VideoFileClip(video_path)\n",
        "\n",
        "        video_duration = video.duration  # Get total duration of the video\n",
        "        clip_count = 1  # To maintain clip order\n",
        "\n",
        "        # Loop through the video in chunks of clip_duration (e.g., 60 seconds)\n",
        "        for start_time in range(0, int(video_duration), clip_duration):\n",
        "            end_time = min(start_time + clip_duration, video_duration)\n",
        "\n",
        "            # Generate the output file name in the format video_1.mp4, video_2.mp4, etc.\n",
        "            output_name = f\"video_{clip_count}.mp4\"\n",
        "            output_path = os.path.join(video_output_folder, output_name)\n",
        "\n",
        "            # Clip the video (from start_time to end_time) without audio\n",
        "            video_clip = video.subclip(start_time, end_time)\n",
        "\n",
        "            # Save the clipped video (without processing audio)\n",
        "            video_clip.write_videofile(output_path, codec=\"libx264\", audio=False)\n",
        "\n",
        "            # Close the video clip to free memory\n",
        "            video_clip.close()\n",
        "\n",
        "            # Increment the clip count\n",
        "            clip_count += 1\n",
        "\n",
        "        # Close the main video\n",
        "        video.close()\n",
        "\n",
        "        print(f\"All clips saved for {video_name} in {video_output_folder}\")\n",
        "    else:\n",
        "        print(f\"Video file {video_path} not found.\")\n",
        "\n",
        "# Function to process all videos in a folder\n",
        "def process_videos_in_folder(video_folder, clip_duration, output_folder):\n",
        "    # Iterate through all video files in the folder\n",
        "    for video_file in os.listdir(video_folder):\n",
        "        video_path = os.path.join(video_folder, video_file)\n",
        "\n",
        "        # Check if the current file is a video file (you can extend this condition to other formats)\n",
        "        if video_file.endswith(('.mp4', '.mov', '.avi', '.mkv', '.m4a')):\n",
        "            clip_video_in_intervals(video_path, clip_duration, output_folder)\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    video_folder = 'video7'\n",
        "    output_folder = '60_clips'\n",
        "    clip_duration = 60\n",
        "\n",
        "    process_videos_in_folder(video_folder, clip_duration, output_folder)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dIqdAFR3ILx5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}