{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLcTQ93ZaOk-"
      },
      "source": [
        "# News Aggregator App | Upgrad Capstone Project"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview**  \n",
        "\n",
        "The News Aggregator App is designed to fetch news articles from various topics, process them, and store them in a structured format. This application utilizes several libraries, including LangChain for natural language processing and Pinecone for vector storage. Below is a detailed breakdown of the code and concepts used in the project."
      ],
      "metadata": {
        "id": "e1yZ9e9-XvMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Installations | Required**  \n",
        "\n",
        "Before running the application, you need to install the required libraries. The following command installs all necessary packages:"
      ],
      "metadata": {
        "id": "n41W70xmaVLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  langchain langchain-community langchain-pinecone langchain-huggingface neo4j langchain-core tiktoken yfiles_jupyter_graphs newsapi-python requests huggingface_hub pinecone-client tqdm pinecone sentence_transformers py2neo gradio fastapi mistralai"
      ],
      "metadata": {
        "id": "DkBPPIrOaZ6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usecases:**\n",
        "- `langchain`: A framework for building applications with language models.\n",
        "- `newsapi-python`: A client for fetching news articles from NewsAPI.\n",
        "- `pinecone-client`: A client for interacting with Pinecone, a vector database.\n",
        "- `fastapi`: A modern web framework for building APIs with Python."
      ],
      "metadata": {
        "id": "eMBtdPcvYAOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries | Required**   \n",
        "The next step involves importing the necessary libraries:"
      ],
      "metadata": {
        "id": "LuNp4ghnbVBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Standard library imports\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import threading\n",
        "from uuid import uuid4\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "# Third-party imports\n",
        "import pinecone\n",
        "import requests\n",
        "import torch\n",
        "from google.colab import output, userdata\n",
        "from neo4j import GraphDatabase\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_core.runnables import (\n",
        "    ConfigurableField,\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from mistralai import Mistral\n",
        "\n",
        "# Frontend - Backend libraries\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from py2neo import Graph, Node, Relationship\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "import threading\n",
        "from typing import List, Dict\n",
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "# Google Colab widget configuration\n",
        "try:\n",
        "    output.enable_custom_widget_manager()\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "Xb4Bm8v5bsDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading environment variables | API keys**  \n",
        "\n",
        "Environment variables are loaded to securely manage API keys. These variables are essential for authenticating with various services like Hugging Face and NewsAPI."
      ],
      "metadata": {
        "id": "nvHqQNbLc3Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGINGFACE_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
        "NEWSAPI_KEY = userdata.get('NEWSAPI_KEY')\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')\n",
        "MISTRAL_API_KEY = userdata.get('MISTRAL_API_KEY')\n",
        "NEO4J_URI = userdata.get('NEO4J_URI')\n",
        "NEO4J_USERNAME = \"neo4j\""
      ],
      "metadata": {
        "id": "mstvz2KddKfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate HuggingFace access token   \n",
        "*(run below command to validate your access token in terminal)*"
      ],
      "metadata": {
        "id": "Fw-qg_43elsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
      ],
      "metadata": {
        "id": "OUdXI3smeiKJ",
        "outputId": "6ffd1322-9b10-4dbf-fbee-4925dc5d94f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `giru-upgrad-news-agg-read-only` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `giru-upgrad-news-agg-read-only`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup OS environment variables**"
      ],
      "metadata": {
        "id": "ZkKgBtKnfPfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HUGGINGFACE_TOKEN'] = HUGGINGFACE_TOKEN\n",
        "os.environ['NEWSAPI_KEY'] = NEWSAPI_KEY\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "os.environ['NEO4J_PASSWORD'] = NEO4J_PASSWORD\n",
        "os.environ['NEO4J_URI'] = NEO4J_URI\n",
        "os.environ['NEO4J_USERNAME'] = NEO4J_USERNAME"
      ],
      "metadata": {
        "id": "WgAyaoEZflOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fetching & storing news articles from `NEWSAPI`**"
      ],
      "metadata": {
        "id": "-9QNnd9yZMDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(text: str) -> int:\n",
        "    \"\"\"Count words in a text string.\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    return len(text.split())"
      ],
      "metadata": {
        "id": "nmdDIdC4UfHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_safe_filename(topic: str, title: str) -> str:\n",
        "    \"\"\"Create a safe filename from topic and title.\"\"\"\n",
        "    # Remove or replace invalid filename characters\n",
        "    invalid_chars = '<>:\"/\\\\|?*'\n",
        "    safe_title = ''.join(c if c not in invalid_chars else '_' for c in title)\n",
        "    safe_title = safe_title[:100]  # Limit length\n",
        "    return f\"{topic}_{safe_title}.txt\""
      ],
      "metadata": {
        "id": "snapcx4UUhO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `fetch_multiple_topics` retrieves articles based on specified topics.\n",
        "- The function creates a folder to store articles if it doesn't exist.\n",
        "- It constructs a URL to fetch articles from `NewsAPI` based on the specified topic.\n",
        "- Each article's title and content are saved into separate text files named after the topic and title."
      ],
      "metadata": {
        "id": "4Zn5Tqv8ZFDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_multiple_topics(api_key: str, topics: List[str], database_folder: str = \"database\") -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Fetch articles for multiple topics and save each article to a separate file.\n",
        "\n",
        "    Args:\n",
        "        api_key: NewsAPI key\n",
        "        topics: List of topics to fetch articles for\n",
        "        database_folder: Folder to store article files\n",
        "        days_from: Number of days from today to fetch articles\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with topics and their saved article counts\n",
        "    \"\"\"\n",
        "    # Ensure database folder exists\n",
        "    os.makedirs(database_folder, exist_ok=True)\n",
        "    article_counts = {topic: 0 for topic in topics}\n",
        "\n",
        "    for topic in topics:\n",
        "        try:\n",
        "            url = (\n",
        "                f\"https://newsapi.org/v2/everything\"\n",
        "                f\"?q={topic}\"\n",
        "                f\"&sortBy=popularity\"\n",
        "                f\"&pageSize=100\"\n",
        "                f\"&apiKey={api_key}\"\n",
        "            )\n",
        "\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            articles = response.json().get('articles', [])\n",
        "\n",
        "            for article in articles:\n",
        "                title = article.get('title', 'No title')\n",
        "                content = article.get('content', '')\n",
        "                description = article.get('description', '')\n",
        "\n",
        "                # Combine content and description for word count\n",
        "                full_text = f\"{content}\\n{description}\".strip()\n",
        "                word_count = count_words(full_text)\n",
        "\n",
        "                # Skip if content is too short\n",
        "                if word_count < 10:\n",
        "                    continue\n",
        "\n",
        "                # Create filename using topic and title\n",
        "                filename = create_safe_filename(topic, title)\n",
        "                filepath = os.path.join(database_folder, filename)\n",
        "\n",
        "                # Write article to file\n",
        "                with open(filepath, 'w', encoding='utf-8') as file:\n",
        "                    # Write metadata header\n",
        "                    file.write(\"=\" * 50 + \"\\n\")\n",
        "                    file.write(f\"Topic: {topic}\\n\")\n",
        "                    file.write(f\"Title: {title}\\n\")\n",
        "                    file.write(f\"Published: {article.get('publishedAt', 'No date')}\\n\")\n",
        "                    file.write(f\"Source: {article.get('source', {}).get('name', 'Unknown')}\\n\")\n",
        "                    file.write(f\"URL: {article.get('url', 'No URL')}\\n\")\n",
        "                    file.write(f\"Word Count: {word_count}\\n\")\n",
        "                    file.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "                    # Write content\n",
        "                    file.write(full_text)\n",
        "\n",
        "                article_counts[topic] += 1\n",
        "\n",
        "            # Sleep to respect API rate limits\n",
        "            time.sleep(1)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching articles for {topic}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return article_counts"
      ],
      "metadata": {
        "id": "vrcW9iPZlngu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Processing Articles with RAG (Retrieval-Augmented Generation)**"
      ],
      "metadata": {
        "id": "uEdD53s3ZqTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `ArticleRAG` class initializes the system to process articles and store embeddings.\n",
        "- The constructor initializes embeddings using Hugging Face and sets up a Pinecone index.\n",
        "- The `process_articles` method reads each article from the specified folder and splits it into chunks before storing them in Pinecone."
      ],
      "metadata": {
        "id": "fvP0_7ViZvo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArticleRAG:\n",
        "    def __init__(self, database_folder: str = \"database\", index_name: str = \"articles-embeddings\"):\n",
        "        \"\"\"\n",
        "        Initialize RAG system using LangChain and HuggingFace embeddings with Pinecone integration.\n",
        "\n",
        "        Args:\n",
        "            database_folder: Folder containing article files\n",
        "            index_name: Name for the Pinecone index\n",
        "        \"\"\"\n",
        "        self.database_folder = database_folder\n",
        "\n",
        "        # Initialize HuggingFace Embeddings\n",
        "        print(\"Initializing HuggingFace Embeddings...\")\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "        # Initialize Pinecone with new pattern\n",
        "        print(\"Initializing Pinecone...\")\n",
        "        self.pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Check if index exists and create if needed\n",
        "        if self.index_name not in self.pc.list_indexes().names():\n",
        "            print(f\"Creating new Pinecone index: {self.index_name}\")\n",
        "            self.pc.create_index(\n",
        "                name=self.index_name,\n",
        "                dimension=self.embeddings.embed_query(\"\").shape[0],\n",
        "                metric='cosine',\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud='aws',\n",
        "                    region='us-east-1'\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Initialize vector store with the new index\n",
        "        self.vector_store = PineconeVectorStore(\n",
        "            index=self.pc.Index(self.index_name),\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "        # Text splitter for chunking\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,  # Adjust for larger chunks\n",
        "            chunk_overlap=100,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "    def process_articles(self):\n",
        "        \"\"\"\n",
        "        Read and process articles from the database folder, storing embeddings in Pinecone.\n",
        "        \"\"\"\n",
        "        print(\"Processing articles from database folder...\")\n",
        "        docs = []\n",
        "        for filename in os.listdir(self.database_folder):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                filepath = os.path.join(self.database_folder, filename)\n",
        "                with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "                    # Read the file content\n",
        "                    content = file.read()\n",
        "\n",
        "                    # Split content into metadata and body\n",
        "                    parts = content.split(\"=\" * 50)\n",
        "                    if len(parts) < 3:\n",
        "                        print(f\"Skipping malformed file: {filename}\")\n",
        "                        continue\n",
        "\n",
        "                    metadata_text = parts[1].strip()\n",
        "                    article_content = parts[2].strip()\n",
        "\n",
        "                    # Chunk the article content\n",
        "                    chunks = self.text_splitter.split_text(article_content)\n",
        "\n",
        "                    # Create Document objects with metadata\n",
        "                    for i, chunk in enumerate(chunks):\n",
        "                        doc = Document(\n",
        "                            page_content=chunk,\n",
        "                            metadata={\n",
        "                                \"source_file\": filename,\n",
        "                                \"chunk_index\": i,\n",
        "                                \"total_chunks\": len(chunks),\n",
        "                                \"metadata_text\": metadata_text\n",
        "                            }\n",
        "                        )\n",
        "                        docs.append(doc)\n",
        "\n",
        "        if docs:\n",
        "            # Generate UUIDs for documents\n",
        "            uuids = [str(uuid4()) for _ in range(len(docs))]\n",
        "\n",
        "            # Add documents to Pinecone\n",
        "            print(\"Adding documents to Pinecone...\")\n",
        "            self.vector_store.add_documents(documents=docs, ids=uuids)\n",
        "        else:\n",
        "            print(\"No valid documents to process.\")\n",
        "\n",
        "    def query(self, query_text: str, k: int = 5):\n",
        "        \"\"\"\n",
        "        Query the Pinecone vector database.\n",
        "\n",
        "        Args:\n",
        "            query_text: Query string\n",
        "            k: Number of top results to return\n",
        "\n",
        "        Returns:\n",
        "            List of relevant results with metadata\n",
        "        \"\"\"\n",
        "        print(\"Querying Pinecone...\")\n",
        "        results = self.vector_store.similarity_search(query_text, k=k)\n",
        "        formatted_results = [\n",
        "            {\n",
        "                \"content\": res.page_content,\n",
        "                \"metadata\": res.metadata\n",
        "            }\n",
        "            for res in results\n",
        "        ]\n",
        "        return formatted_results"
      ],
      "metadata": {
        "id": "77mX_tXTj9gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = NEWSAPI_KEY  # Replace with your API key\n",
        "topics = [\"Artificial Intelligence\", \"Politics\", \"Business\", \"Technology\", \"Sports\", \"Entertainment\", \"Health\"]\n",
        "\n",
        "print(\"Fetching articles...\")\n",
        "results = fetch_multiple_topics(api_key=API_KEY, topics=topics, database_folder=\"database\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nArticles saved:\")\n",
        "for topic, count in results.items():\n",
        "    print(f\"{topic}: {count} articles\")\n",
        "\n",
        "# Initialize and process articles with ArticleRAG\n",
        "rag = ArticleRAG(database_folder=\"database\", index_name=\"articles-embeddings\")\n",
        "rag.process_articles()"
      ],
      "metadata": {
        "id": "Be3hHQD9l5cp",
        "outputId": "9814c190-6b42-4cdd-9978-ebf7d33ed305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching articles...\n",
            "\n",
            "Articles saved:\n",
            "Artificial Intelligence: 96 articles\n",
            "Politics: 98 articles\n",
            "Business: 90 articles\n",
            "Technology: 86 articles\n",
            "Sports: 93 articles\n",
            "Entertainment: 83 articles\n",
            "Health: 90 articles\n",
            "Initializing HuggingFace Embeddings...\n",
            "Initializing Pinecone...\n",
            "Processing articles from database folder...\n",
            "Adding documents to Pinecone...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backend**"
      ],
      "metadata": {
        "id": "sfTW-_OIZ_T1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Querying Database**  \n",
        "The `query` method allows users to search through stored articles. This method performs a similarity search based on the user's query and retrieves relevant documents along with their metadata."
      ],
      "metadata": {
        "id": "QcIxcJgZaYL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FastAPI | Backend**  \n",
        "The `FastAPI` app defines an endpoint `/fetch-news/` where users can send requests to fetch news based on categories. The `FetchNewsRequest` model validates incoming requests."
      ],
      "metadata": {
        "id": "0eSzHDM1amS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI()\n",
        "\n",
        "class FetchNewsRequest(BaseModel):\n",
        "    user_id: str\n",
        "    category: str\n",
        "\n",
        "def fetch_documents(query: str) -> str:\n",
        "    # Initialize docs_content inside the function\n",
        "    docs_content = ''\n",
        "    try:\n",
        "        results = rag.query(query, k=5)\n",
        "        for result in results:\n",
        "            docs_content += result['metadata']\n",
        "            docs_content += '\\n'\n",
        "            docs_content += result['content']\n",
        "            docs_content += '\\n'\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching documents: {str(e)}\")\n",
        "    return docs_content\n",
        "\n",
        "def combine_preferences_and_documents(preference: str, docs_content: str) -> dict:\n",
        "    return {\n",
        "        \"user_preferences\": [preference],  # Changed to list to match template\n",
        "        \"documents\": [{\"title\": \"Document\", \"content\": docs_content}]  # Changed to match template\n",
        "    }\n",
        "\n",
        "def generate_prompt(combined_data: dict) -> str:\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"user_preferences\", \"documents\"],\n",
        "        template=\"\"\"\n",
        "        User Preferences: {user_preferences}\n",
        "        Documents:\n",
        "        {documents}\n",
        "        Based on the above preferences and documents, please provide a summary or insights tailored to the user's interests. Don't add the information that it's generated by you.\n",
        "        Notes:\n",
        "        1. Write your content like it's reported by a TV reporter.\n",
        "        2. Don't output in markdown or HTML document, just give a final output as single paragraph.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    documents_str = \"\\n\".join([f\"- {doc['title']}: {doc['content']}\" for doc in combined_data[\"documents\"]])\n",
        "\n",
        "    prompt = prompt_template.format(\n",
        "        user_preferences=\", \".join(combined_data[\"user_preferences\"]),\n",
        "        documents=documents_str\n",
        "    )\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def llm_response(prompt: str) -> str:\n",
        "    model = \"mistral-large-latest\"\n",
        "    client = Mistral(api_key = MISTRAL_API_KEY)\n",
        "    response = client.chat.complete(\n",
        "        model= model,\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "@app.post(\"/fetch_news/\")\n",
        "def fetch_news(request: FetchNewsRequest):\n",
        "    try:\n",
        "        query = f'Top news articles that match these {request.category}, including their titles, summaries, and publication dates and what is going on, nowadays, on the topic {request.category}. News around {request.category}. What is latest news in {request.category}?.'\n",
        "\n",
        "        docs_content = fetch_documents(query)\n",
        "        combined_data = combine_preferences_and_documents(request.category, docs_content)\n",
        "        prompt = generate_prompt(combined_data)\n",
        "        content = llm_response(prompt)\n",
        "\n",
        "        news = [\n",
        "            {\"title\": f\"Latest in {request.category}\", \"content\": content}\n",
        "        ]\n",
        "        return {\"news\": news}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}, 500\n",
        "\n",
        "# Run FastAPI server in a thread\n",
        "def run_fastapi():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8010)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    threading.Thread(target=run_fastapi, daemon=True).start()"
      ],
      "metadata": {
        "id": "dsTGPsa1l74c",
        "outputId": "d7637901-fce4-40d4-bb76-65fc383e5cef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [10418]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8010 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frontend**"
      ],
      "metadata": {
        "id": "yh6CaAJBq5nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend_url = \"http://0.0.0.0:8010/\"  # Ensure this matches your backend's actual URL\n",
        "\n",
        "# Function to send data to backend and fetch news\n",
        "def fetch_news(user_id, category):\n",
        "    payload = {\n",
        "        \"user_id\": user_id,\n",
        "        \"category\": category\n",
        "    }\n",
        "    try:\n",
        "        # Backend endpoint to fetch news\n",
        "        response = requests.post(f\"{backend_url}/fetch_news/\", json=payload)\n",
        "        if response.status_code == 200:\n",
        "            news = response.json().get(\"news\", [])\n",
        "            cards = \"\\n\\n\".join([f\"{n['title']}\\n{n['content']}\" for n in news])\n",
        "            return cards\n",
        "        else:\n",
        "            return f\"Error fetching news! (Status Code: {response.status_code})\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Gradio UI\n",
        "users = [\"1\", \"2\", \"3\"]\n",
        "categories = [\n",
        "    \"Artificial Intelligence\", \"Politics\", \"Business\", \"Technology\",\n",
        "    \"Sports\", \"Entertainment\", \"Health\"\n",
        "]\n",
        "\n",
        "with gr.Blocks() as ui:\n",
        "    # User selection\n",
        "    user_dropdown = gr.Dropdown(users, label=\"Select User\", value=\"1\")\n",
        "    # Category selection\n",
        "    category_dropdown = gr.Dropdown(categories, label=\"Select Category\")\n",
        "    # Button to fetch news\n",
        "    fetch_button = gr.Button(\"Fetch News\")\n",
        "    # News display area\n",
        "    news_display = gr.Textbox(label=\"News\", interactive=False, placeholder=\"News will appear here\")\n",
        "\n",
        "    # Button click event\n",
        "    fetch_button.click(\n",
        "        fn=fetch_news,\n",
        "        inputs=[user_dropdown, category_dropdown],\n",
        "        outputs=news_display\n",
        "    )\n",
        "\n",
        "ui.launch()\n"
      ],
      "metadata": {
        "id": "FZVaTTNyq5lP",
        "outputId": "2492b527-dce9-452a-cd7a-6951907cbb52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5ce228d719541dfbdb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5ce228d719541dfbdb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------\n",
        "*Submitted by -*  \n",
        "**Giriraj**  \n",
        "**Data Scientist - Trainee**  \n",
        "*`giriraj@incedoinc.com`*  \n",
        "\n",
        "--------------------------------------------------\n",
        ""
      ],
      "metadata": {
        "id": "h8ZPTbi8a-QU"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}